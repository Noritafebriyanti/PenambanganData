{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"TUGAS/","text":"KUMPULAN TUGAS TUGAS 1.0 from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv('datanorita.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } berat badan tinggi badan tekanan darah lingkar perut 0 80 163 106 61 1 51 180 117 40 2 43 150 106 61 3 65 156 105 58 4 48 150 113 40 5 76 155 113 58 6 45 158 126 62 7 54 158 129 63 8 65 153 106 58 9 66 153 104 45 10 73 169 114 57 11 53 151 119 62 12 74 167 110 53 13 48 173 118 46 14 68 179 124 48 15 41 163 104 70 16 54 163 128 55 17 45 155 112 42 18 71 154 116 60 19 72 162 110 60 20 62 167 122 58 21 73 168 116 69 22 45 158 100 66 23 48 151 116 61 24 63 172 106 62 25 61 175 110 57 26 76 166 102 70 27 77 153 129 41 28 73 164 128 50 29 51 159 123 67 ... ... ... ... ... 70 68 175 123 47 71 53 166 113 42 72 58 174 107 56 73 67 175 121 40 74 76 153 121 56 75 57 159 128 44 76 55 154 127 62 77 51 150 129 51 78 66 160 120 60 79 44 161 128 56 80 47 178 119 66 81 60 159 110 47 82 76 166 117 57 83 58 171 123 67 84 56 151 108 46 85 50 165 130 46 86 67 158 104 53 87 63 158 111 68 88 66 157 127 58 89 43 151 117 51 90 61 173 128 48 91 42 172 124 64 92 53 152 104 51 93 46 161 112 42 94 76 164 115 41 95 79 160 114 63 96 73 159 107 41 97 42 153 127 59 98 44 161 127 63 99 54 163 124 68 100 rows \u00d7 4 columns from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"count()\"]+[df[col].count() for col in df.columns], [\"mean()\"]+[df[col].mean() for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) method berat badan tinggi badan tekanan darah lingkar perut count() 100 100 100 100 mean() 59.47 163.96 115.97 55.77 std() 11.32 9.06 8.66 9.22 min() 40 150 100 40 max() 80 180 130 70 q1() 51.0 156.75 109.0 48.0 q2() 61.0 163.0 116.0 57.0 q3() 67.25 172.25 123.0 63.0 skew() 0.06 0.15 0.00 -0.21 TUGAS 2.0 Pengertian Data Mining Data Mining adalah proses memilah-milah sejumlah data yang berkaitan (data set) untuk mengidentifikasi pola dan membangun hubungan untuk memecahkan masalah melalui analisa data. Alat data mining memungkinkan individu atau perusahaan untuk memprediksi tren masa depan. Jika diartikan atau diterjemahkan secara harfiah, data mining adalah pengemangan data. Kalimat lain untuk menjelaskannya, data mining adalah proses menemukan anomali, pola dan korelasi diantara berbagai data set yang besar untuk memprediksi hasil. Dengan menggunakan berbagai teknik, kita dapat menggunakan informasi yang digali untuk meningkatkan pendapatan, efisiensi, meningkatkan pelayanan pelanggan, mengurangi risiko, dan masih banyak lagi. Manfaat Data Mining Jika dijelaskan secara umum, manfaat data mining adalah untuk mengungkap pola yang tersembunyi dan hubungan dalam data yang dapat digunakan untuk membuat prediksi yang bermanfaat untuk bisnis. Manfaat data mining secara spesifik sangat beragam tergantung tujuan dan industri yang dijalankan. Bagian penjualan dan pemasaran dapat menggali data pelanggan untuk meningkatkan tingkat konversi prospek atau untuk membuat materi pemasaran one-to-one. Informasi data mining mengenai pola sejarah penjualan dan perilaku pelanggan dapat digunakan untuk membangun model prediksi yang bermanfaat untuk penjualan di masa mendatang, membuat produk atau layanan baru. Perusahaan dalam industri keuangan menggunakan alat data mining untuk membangun model risiko dan mendeteksi penipuan. Industri manufaktur menggunakan alat data mining untuk meningkatkan keamanan produk, mengidentifikasi masalah kualitas, mengelola rantai pasokan barang dan meningkatkan operasional. Contoh Data Mining Berikut adalah contoh data mining yang sederhana. Bisa dijadikan sebagai inspirasi untuk meningkatkan dan menyempurnakan strategi pemasaran dan unggul dalam persaingan bisnis. menhitung jarak from IPython.display import Image, display ; display(Image(filename=\"chord.PNG\")) menghitung jarak biner from IPython.display import Image, display ; display(Image(filename=\"menghitung jarak biner.PNG\")) menghitung catagorical from IPython.display import Image, display ; display(Image(filename=\"categorical.PNG\")) menghitung jarak ordinal from IPython.display import Image, display ; display(Image(filename=\"ordinal.PNG\")) menghitung jarak campuran from IPython.display import Image, display ; display(Image(filename=\"campuran.PNG\")) Mengukur Jarak Data Tugas II Mengukur Jarak Data from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv('databaru.csv') k=df.iloc[10:16] k .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } umur istri pendidikan istri pendidikan suami jumlah anak agama istri perkerjaan istri kependudukan suami taraf kehidupan paparan media 10 38 1 3 2 1 0 3 3 1 11 42 1 4 4 1 1 1 3 0 12 44 4 4 1 1 0 1 4 0 13 42 2 4 1 1 0 3 3 0 14 38 3 4 2 1 1 2 3 0 15 26 2 4 0 1 1 4 1 0 numerical=[0,3] categorical=[1,2,6,7] binary=[4,5,8] ordinal=[1,2] from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[0]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[0]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[0]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[0]+[0]+[0]+[0], [\"v4-v5\"]+[0]+[0]+[0]+[0]+[0], [\"v5-v6\"]+[0]+[0]+[0]+[0]+[0] ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 0 0 0 0 v1-v3 0 0 0 0 0 v2-v3 0 0 0 0 0 v3-v4 0 0 0 0 0 v4-v5 0 0 0 0 0 v5-v6 0 0 0 0 0 Jarak numeric def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(k.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(k.values.tolist()[v1][jnis[x]])**2) jmlh=jmlh+(int(k.values.tolist()[v1][jnis[x]])*int(k.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[0]+[0]+[0], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 0 0 0 v1-v3 0 1.41 0 0 0 v2-v3 0 1.41 0 0 0 v3-v4 0 1.41 0 0 0 v4-v5 0 1.41 0 0 0 v5-v6 0 1.41 0 0 0 Jarak Ordinal def ordDist(v1,v2,jns): jmlh=0 for x in range (len(jns)): z1=int(k.values.tolist()[v1][jns[x]])-1 z2=int(k.values.tolist()[v2][jns[x]])-1 jmlh=jmlh+chordDist(z1,z2,jns) return (jmlh) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[\"{:.2f}\".format(ordDist(0,1,ordinal))]+[categoricalDist(0,1,categorical)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[\"{:.2f}\".format(ordDist(0,2,ordinal))]+[categoricalDist(0,2,categorical)]+[binaryDist(0,1,binary)], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[\"{:.2f}\".format(ordDist(1,2,ordinal))]+[categoricalDist(1,2,categorical)]+[binaryDist(0,1,binary)], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[\"{:.2f}\".format(ordDist(2,3,ordinal))]+[categoricalDist(2,3,categorical)]+[binaryDist(0,1,binary)], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[\"{:.2f}\".format(ordDist(3,4,ordinal))]+[categoricalDist(3,4,categorical)]+[binaryDist(0,1,binary)], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(4,5,numerical))]+[\"{:.2f}\".format(ordDist(4,5,ordinal))]+[categoricalDist(4,5,categorical)]+[binaryDist(0,1,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 2.74 2 0.0 v1-v3 0 1.41 2.71 4 0.0 v2-v3 0 1.41 2.69 2 0.0 v3-v4 0 1.41 2.76 3 0.0 v4-v5 0 1.41 2.74 2 0.0 v5-v6 0 1.41 2.78 3 0.0 Jarak categorical def categoricalDist(v1,v2,jnis): jmlh=0 for x in range (len(jnis)): if (k.values.tolist()[v1][jnis[x]])!=(k.values.tolist()[v2][jnis[x]]): jmlh=jmlh+1 return (jmlh) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[\"{:.2f}\".format(ordDist(0,1,ordinal))]+[categoricalDist(0,1,categorical)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[\"{:.2f}\".format(ordDist(0,2,ordinal))]+[categoricalDist(0,2,categorical)]+[binaryDist(0,1,binary)], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[\"{:.2f}\".format(ordDist(1,2,ordinal))]+[categoricalDist(1,2,categorical)]+[binaryDist(0,1,binary)], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[\"{:.2f}\".format(ordDist(2,3,ordinal))]+[categoricalDist(2,3,categorical)]+[binaryDist(0,1,binary)], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[\"{:.2f}\".format(ordDist(3,4,ordinal))]+[categoricalDist(3,4,categorical)]+[binaryDist(0,1,binary)], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(4,5,numerical))]+[\"{:.2f}\".format(ordDist(4,5,ordinal))]+[categoricalDist(4,5,categorical)]+[binaryDist(0,1,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 2.74 2 0.0 v1-v3 0 1.41 2.71 4 0.0 v2-v3 0 1.41 2.69 2 0.0 v3-v4 0 1.41 2.76 3 0.0 v4-v5 0 1.41 2.74 2 0.0 v5-v6 0 1.41 2.78 3 0.0 Jarak binary def binaryDist(v1,v2,jnis): q=0 r=0 s=0 t=0 for x in range (len(jnis)): if (int(k.values.tolist()[v1][jnis[x]]))==1 and (int(k.values.tolist()[v2][jnis[x]]))==1: q=q+1 elif (int(k.values.tolist()[v1][jnis[x]]))==1 and (int(k.values.tolist()[v2][jnis[x]]))==2: r=r+1 elif (int(k.values.tolist()[v1][jnis[x]]))==2 and (int(k.values.tolist()[v2][jnis[x]]))==1: s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t)) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[\"{:.2f}\".format(ordDist(0,1,ordinal))]+[categoricalDist(0,1,categorical)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[\"{:.2f}\".format(ordDist(0,2,ordinal))]+[categoricalDist(0,2,categorical)]+[binaryDist(0,1,binary)], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[\"{:.2f}\".format(ordDist(1,2,ordinal))]+[categoricalDist(1,2,categorical)]+[binaryDist(0,1,binary)], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[\"{:.2f}\".format(ordDist(2,3,ordinal))]+[categoricalDist(2,3,categorical)]+[binaryDist(0,1,binary)], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[\"{:.2f}\".format(ordDist(3,4,ordinal))]+[categoricalDist(3,4,categorical)]+[binaryDist(0,1,binary)], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(4,5,numerical))]+[\"{:.2f}\".format(ordDist(4,5,ordinal))]+[categoricalDist(4,5,categorical)]+[binaryDist(0,1,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 2.74 2 0.0 v1-v3 0 1.41 2.71 4 0.0 v2-v3 0 1.41 2.69 2 0.0 v3-v4 0 1.41 2.76 3 0.0 v4-v5 0 1.41 2.74 2 0.0 v5-v6 0 1.41 2.78 3 0.0 Jarak campuran def jarak(v1,v2): return ((chordDist(v1,v2,numerical)+ordDist(v1,v2,ordinal)+categoricalDist(v1,v2,categorical)+binaryDist(v1,v2,binary))/4) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[\"{:.2f}\".format(ordDist(0,1,ordinal))]+[categoricalDist(0,1,categorical)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[\"{:.2f}\".format(ordDist(0,2,ordinal))]+[categoricalDist(0,2,categorical)]+[binaryDist(0,1,binary)], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[\"{:.2f}\".format(ordDist(1,2,ordinal))]+[categoricalDist(1,2,categorical)]+[binaryDist(0,1,binary)], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[\"{:.2f}\".format(ordDist(2,3,ordinal))]+[categoricalDist(2,3,categorical)]+[binaryDist(0,1,binary)], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[\"{:.2f}\".format(ordDist(3,4,ordinal))]+[categoricalDist(3,4,categorical)]+[binaryDist(0,1,binary)], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(4,5,numerical))]+[\"{:.2f}\".format(ordDist(4,5,ordinal))]+[categoricalDist(4,5,categorical)]+[binaryDist(0,1,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 2.74 2 0.0 v1-v3 0 1.41 2.71 4 0.0 v2-v3 0 1.41 2.69 2 0.0 v3-v4 0 1.41 2.76 3 0.0 v4-v5 0 1.41 2.74 2 0.0 v5-v6 0 1.41 2.78 3 0.0 TUGAS 3.0 Naive Bayes Classifier Classifier adalah model machine learning yang digunakan untuk membedakan objek berdasarkan fitur tertentu. Naive Bayes Classifier adalah machine learning yang menggunakan probabilitas untuk mengklasfikasi objek dimana untuk setiap fitur $X$ sejumlah $n$ : $$ P(\\operatorname{C_k}) = \\frac{\\left(\\prod_{i=1}^n P(X_i|C_k)\\right)P(C_k)}{P(X)} $$ $P$ adalah probabilitas yang muncul. Untuk data numerik $P$ adalah: $$ P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) $$ dimana $v$ adalah nilai dalam fitur, $\\sigma_k$ adalah Standar deviasi dan $\\mu_k$ adalah Rata-rata untuk K (kolom) Langkah-Langkah Training 1. Ambil data set from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML, display; from tabulate import tabulate def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) # IRIS TRAINING TABLE iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.2 3.5 1.5 0.2 setosa 5 2.3 3.3 1 versicolor 7.7 2.6 6.9 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 4.3 3 1.1 0.1 setosa 4.9 2.4 3.3 1 versicolor 5.5 2.4 3.7 1 versicolor 6.7 3.1 4.7 1.5 versicolor 5 3.5 1.6 0.6 setosa 6.1 3 4.9 1.8 virginica 5 3.5 1.3 0.3 setosa 6.9 3.2 5.7 2.3 virginica 4.9 3 1.4 0.2 setosa 7.2 3.2 6 1.8 virginica 5.4 3 4.5 1.5 versicolor 5.6 2.7 4.2 1.3 versicolor 4.6 3.2 1.4 0.2 setosa 5.8 2.6 4 1.2 versicolor 4.8 3 1.4 0.3 setosa 4.8 3 1.4 0.1 setosa 7.6 3 6.6 2.1 virginica 5.7 4.4 1.5 0.4 setosa 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 7.7 3 6.1 2.3 virginica 5.7 2.9 4.2 1.3 versicolor 6.7 3 5 1.7 versicolor 4.7 3.2 1.6 0.2 setosa 5.4 3.4 1.7 0.2 setosa 2. Sampel data untuk di tes test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4] 3. Identifikasi Per Grup Class Target untuk data Training dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.18333333 3.56666667 1.51666667 0.31666667] Sigma_s => [0.3250641 0.22509257 0.14719601 0.1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa versicolor ===> Mu_s => [5.89166667 2.76666667 4.13333333 1.26666667] Sigma_s => [0.52476546 0.33393884 0.46188022 0.21461735] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor virginica ===> Mu_s => [6.61666667 3.13333333 5.58333333 2.06666667] Sigma_s => [0.7790826 0.34465617 0.59670814 0.23868326] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica 5. Hitung Probabilitas Prior dan Likehood WIP: Probabilitas Evidence masukkan ke hitungan def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4 6. Rank & Tarik Kesimpulan Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136 print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica Setelah kita sudah menghitung untuk data training kita, kita akan lakukan test lagi untuk data asli kita # ONE FUNCTION FOR CLASSIFIER def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica Kesimpulan corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 144 of 150 == 96.000000 % TUGAS 4.0 Naive Bayes Classifier Naive Bayes Classifier adalah classifier dimana untuk setiap fitur $X$ sejumlah $n$ : $$ P(\\operatorname{C_k}) = \\frac{\\left(\\prod_{i=1}^n P(X_i|C_k)\\right)P(C_k)}{P(X)} $$ Layman terms: $$ \\operatorname{posterior} = \\frac{\\operatorname{prior}\\times\\operatorname{likehood}}{\\operatorname{evidence}} $$ $P$ adalah probabilitas yang muncul. Untuk data numerik $P$ adalah: $$ P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) $$ dimana $v$ adalah nilai dalam fitur, $\\sigma_k$ adalah Standar deviasi dan $\\mu_k$ adalah Rata-rata untuk K (kolom) Langkah-Langkah Training 1. Ambil data set from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML, display; from tabulate import tabulate def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) # IRIS TRAINING TABLE iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.2 3.5 1.5 0.2 setosa 5 2.3 3.3 1 versicolor 7.7 2.6 6.9 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 4.3 3 1.1 0.1 setosa 4.9 2.4 3.3 1 versicolor 5.5 2.4 3.7 1 versicolor 6.7 3.1 4.7 1.5 versicolor 5 3.5 1.6 0.6 setosa 6.1 3 4.9 1.8 virginica 5 3.5 1.3 0.3 setosa 6.9 3.2 5.7 2.3 virginica 4.9 3 1.4 0.2 setosa 7.2 3.2 6 1.8 virginica 5.4 3 4.5 1.5 versicolor 5.6 2.7 4.2 1.3 versicolor 4.6 3.2 1.4 0.2 setosa 5.8 2.6 4 1.2 versicolor 4.8 3 1.4 0.3 setosa 4.8 3 1.4 0.1 setosa 7.6 3 6.6 2.1 virginica 5.7 4.4 1.5 0.4 setosa 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 7.7 3 6.1 2.3 virginica 5.7 2.9 4.2 1.3 versicolor 6.7 3 5 1.7 versicolor 4.7 3.2 1.6 0.2 setosa 5.4 3.4 1.7 0.2 setosa 2. Sampel data untuk di tes test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4] 3. Identifikasi Per Grup Class Target untuk data Training dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.18333333 3.56666667 1.51666667 0.31666667] Sigma_s => [0.3250641 0.22509257 0.14719601 0.1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa versicolor ===> Mu_s => [5.89166667 2.76666667 4.13333333 1.26666667] Sigma_s => [0.52476546 0.33393884 0.46188022 0.21461735] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor virginica ===> Mu_s => [6.61666667 3.13333333 5.58333333 2.06666667] Sigma_s => [0.7790826 0.34465617 0.59670814 0.23868326] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica 5. Hitung Probabilitas Prior dan Likehood WIP: Probabilitas Evidence masukkan ke hitungan def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4 6. Rank & Tarik Kesimpulan Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136 print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica Real Test diberikan variabel dataset dan dataset_classes yang sebagai 'data training' kita, mari kita lakukan itu untuk real Iris data: # ONE FUNCTION FOR CLASSIFIER def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica Kesimpulan corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 144 of 150 == 96.000000 % TUGAS 5.0","title":"TUGAS"},{"location":"TUGAS/#kumpulan-tugas","text":"","title":"KUMPULAN TUGAS"},{"location":"TUGAS/#tugas-10","text":"from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv('datanorita.csv') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } berat badan tinggi badan tekanan darah lingkar perut 0 80 163 106 61 1 51 180 117 40 2 43 150 106 61 3 65 156 105 58 4 48 150 113 40 5 76 155 113 58 6 45 158 126 62 7 54 158 129 63 8 65 153 106 58 9 66 153 104 45 10 73 169 114 57 11 53 151 119 62 12 74 167 110 53 13 48 173 118 46 14 68 179 124 48 15 41 163 104 70 16 54 163 128 55 17 45 155 112 42 18 71 154 116 60 19 72 162 110 60 20 62 167 122 58 21 73 168 116 69 22 45 158 100 66 23 48 151 116 61 24 63 172 106 62 25 61 175 110 57 26 76 166 102 70 27 77 153 129 41 28 73 164 128 50 29 51 159 123 67 ... ... ... ... ... 70 68 175 123 47 71 53 166 113 42 72 58 174 107 56 73 67 175 121 40 74 76 153 121 56 75 57 159 128 44 76 55 154 127 62 77 51 150 129 51 78 66 160 120 60 79 44 161 128 56 80 47 178 119 66 81 60 159 110 47 82 76 166 117 57 83 58 171 123 67 84 56 151 108 46 85 50 165 130 46 86 67 158 104 53 87 63 158 111 68 88 66 157 127 58 89 43 151 117 51 90 61 173 128 48 91 42 172 124 64 92 53 152 104 51 93 46 161 112 42 94 76 164 115 41 95 79 160 114 63 96 73 159 107 41 97 42 153 127 59 98 44 161 127 63 99 54 163 124 68 100 rows \u00d7 4 columns from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"count()\"]+[df[col].count() for col in df.columns], [\"mean()\"]+[df[col].mean() for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) method berat badan tinggi badan tekanan darah lingkar perut count() 100 100 100 100 mean() 59.47 163.96 115.97 55.77 std() 11.32 9.06 8.66 9.22 min() 40 150 100 40 max() 80 180 130 70 q1() 51.0 156.75 109.0 48.0 q2() 61.0 163.0 116.0 57.0 q3() 67.25 172.25 123.0 63.0 skew() 0.06 0.15 0.00 -0.21","title":"TUGAS 1.0"},{"location":"TUGAS/#tugas-20","text":"","title":"TUGAS 2.0"},{"location":"TUGAS/#pengertian-data-mining","text":"Data Mining adalah proses memilah-milah sejumlah data yang berkaitan (data set) untuk mengidentifikasi pola dan membangun hubungan untuk memecahkan masalah melalui analisa data. Alat data mining memungkinkan individu atau perusahaan untuk memprediksi tren masa depan. Jika diartikan atau diterjemahkan secara harfiah, data mining adalah pengemangan data. Kalimat lain untuk menjelaskannya, data mining adalah proses menemukan anomali, pola dan korelasi diantara berbagai data set yang besar untuk memprediksi hasil. Dengan menggunakan berbagai teknik, kita dapat menggunakan informasi yang digali untuk meningkatkan pendapatan, efisiensi, meningkatkan pelayanan pelanggan, mengurangi risiko, dan masih banyak lagi.","title":"Pengertian Data Mining"},{"location":"TUGAS/#manfaat-data-mining","text":"Jika dijelaskan secara umum, manfaat data mining adalah untuk mengungkap pola yang tersembunyi dan hubungan dalam data yang dapat digunakan untuk membuat prediksi yang bermanfaat untuk bisnis. Manfaat data mining secara spesifik sangat beragam tergantung tujuan dan industri yang dijalankan. Bagian penjualan dan pemasaran dapat menggali data pelanggan untuk meningkatkan tingkat konversi prospek atau untuk membuat materi pemasaran one-to-one. Informasi data mining mengenai pola sejarah penjualan dan perilaku pelanggan dapat digunakan untuk membangun model prediksi yang bermanfaat untuk penjualan di masa mendatang, membuat produk atau layanan baru. Perusahaan dalam industri keuangan menggunakan alat data mining untuk membangun model risiko dan mendeteksi penipuan. Industri manufaktur menggunakan alat data mining untuk meningkatkan keamanan produk, mengidentifikasi masalah kualitas, mengelola rantai pasokan barang dan meningkatkan operasional.","title":"Manfaat Data Mining"},{"location":"TUGAS/#contoh-data-mining","text":"Berikut adalah contoh data mining yang sederhana. Bisa dijadikan sebagai inspirasi untuk meningkatkan dan menyempurnakan strategi pemasaran dan unggul dalam persaingan bisnis.","title":"Contoh Data Mining"},{"location":"TUGAS/#menhitung-jarak","text":"from IPython.display import Image, display ; display(Image(filename=\"chord.PNG\"))","title":"menhitung jarak"},{"location":"TUGAS/#menghitung-jarak-biner","text":"from IPython.display import Image, display ; display(Image(filename=\"menghitung jarak biner.PNG\"))","title":"menghitung jarak biner"},{"location":"TUGAS/#menghitung-catagorical","text":"from IPython.display import Image, display ; display(Image(filename=\"categorical.PNG\"))","title":"menghitung catagorical"},{"location":"TUGAS/#menghitung-jarak-ordinal","text":"from IPython.display import Image, display ; display(Image(filename=\"ordinal.PNG\"))","title":"menghitung jarak ordinal"},{"location":"TUGAS/#menghitung-jarak-campuran","text":"from IPython.display import Image, display ; display(Image(filename=\"campuran.PNG\"))","title":"menghitung jarak campuran"},{"location":"TUGAS/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"TUGAS/#tugas-ii-mengukur-jarak-data","text":"from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv('databaru.csv') k=df.iloc[10:16] k .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } umur istri pendidikan istri pendidikan suami jumlah anak agama istri perkerjaan istri kependudukan suami taraf kehidupan paparan media 10 38 1 3 2 1 0 3 3 1 11 42 1 4 4 1 1 1 3 0 12 44 4 4 1 1 0 1 4 0 13 42 2 4 1 1 0 3 3 0 14 38 3 4 2 1 1 2 3 0 15 26 2 4 0 1 1 4 1 0 numerical=[0,3] categorical=[1,2,6,7] binary=[4,5,8] ordinal=[1,2] from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[0]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[0]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[0]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[0]+[0]+[0]+[0], [\"v4-v5\"]+[0]+[0]+[0]+[0]+[0], [\"v5-v6\"]+[0]+[0]+[0]+[0]+[0] ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 0 0 0 0 v1-v3 0 0 0 0 0 v2-v3 0 0 0 0 0 v3-v4 0 0 0 0 0 v4-v5 0 0 0 0 0 v5-v6 0 0 0 0 0","title":"Tugas II Mengukur Jarak Data"},{"location":"TUGAS/#jarak-numeric","text":"def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(k.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(k.values.tolist()[v1][jnis[x]])**2) jmlh=jmlh+(int(k.values.tolist()[v1][jnis[x]])*int(k.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[0]+[0]+[0], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 0 0 0 v1-v3 0 1.41 0 0 0 v2-v3 0 1.41 0 0 0 v3-v4 0 1.41 0 0 0 v4-v5 0 1.41 0 0 0 v5-v6 0 1.41 0 0 0","title":"Jarak numeric"},{"location":"TUGAS/#jarak-ordinal","text":"def ordDist(v1,v2,jns): jmlh=0 for x in range (len(jns)): z1=int(k.values.tolist()[v1][jns[x]])-1 z2=int(k.values.tolist()[v2][jns[x]])-1 jmlh=jmlh+chordDist(z1,z2,jns) return (jmlh) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[\"{:.2f}\".format(ordDist(0,1,ordinal))]+[categoricalDist(0,1,categorical)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[\"{:.2f}\".format(ordDist(0,2,ordinal))]+[categoricalDist(0,2,categorical)]+[binaryDist(0,1,binary)], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[\"{:.2f}\".format(ordDist(1,2,ordinal))]+[categoricalDist(1,2,categorical)]+[binaryDist(0,1,binary)], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[\"{:.2f}\".format(ordDist(2,3,ordinal))]+[categoricalDist(2,3,categorical)]+[binaryDist(0,1,binary)], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[\"{:.2f}\".format(ordDist(3,4,ordinal))]+[categoricalDist(3,4,categorical)]+[binaryDist(0,1,binary)], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(4,5,numerical))]+[\"{:.2f}\".format(ordDist(4,5,ordinal))]+[categoricalDist(4,5,categorical)]+[binaryDist(0,1,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 2.74 2 0.0 v1-v3 0 1.41 2.71 4 0.0 v2-v3 0 1.41 2.69 2 0.0 v3-v4 0 1.41 2.76 3 0.0 v4-v5 0 1.41 2.74 2 0.0 v5-v6 0 1.41 2.78 3 0.0","title":"Jarak Ordinal"},{"location":"TUGAS/#jarak-categorical","text":"def categoricalDist(v1,v2,jnis): jmlh=0 for x in range (len(jnis)): if (k.values.tolist()[v1][jnis[x]])!=(k.values.tolist()[v2][jnis[x]]): jmlh=jmlh+1 return (jmlh) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[\"{:.2f}\".format(ordDist(0,1,ordinal))]+[categoricalDist(0,1,categorical)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[\"{:.2f}\".format(ordDist(0,2,ordinal))]+[categoricalDist(0,2,categorical)]+[binaryDist(0,1,binary)], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[\"{:.2f}\".format(ordDist(1,2,ordinal))]+[categoricalDist(1,2,categorical)]+[binaryDist(0,1,binary)], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[\"{:.2f}\".format(ordDist(2,3,ordinal))]+[categoricalDist(2,3,categorical)]+[binaryDist(0,1,binary)], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[\"{:.2f}\".format(ordDist(3,4,ordinal))]+[categoricalDist(3,4,categorical)]+[binaryDist(0,1,binary)], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(4,5,numerical))]+[\"{:.2f}\".format(ordDist(4,5,ordinal))]+[categoricalDist(4,5,categorical)]+[binaryDist(0,1,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 2.74 2 0.0 v1-v3 0 1.41 2.71 4 0.0 v2-v3 0 1.41 2.69 2 0.0 v3-v4 0 1.41 2.76 3 0.0 v4-v5 0 1.41 2.74 2 0.0 v5-v6 0 1.41 2.78 3 0.0","title":"Jarak categorical"},{"location":"TUGAS/#jarak-binary","text":"def binaryDist(v1,v2,jnis): q=0 r=0 s=0 t=0 for x in range (len(jnis)): if (int(k.values.tolist()[v1][jnis[x]]))==1 and (int(k.values.tolist()[v2][jnis[x]]))==1: q=q+1 elif (int(k.values.tolist()[v1][jnis[x]]))==1 and (int(k.values.tolist()[v2][jnis[x]]))==2: r=r+1 elif (int(k.values.tolist()[v1][jnis[x]]))==2 and (int(k.values.tolist()[v2][jnis[x]]))==1: s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t)) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[\"{:.2f}\".format(ordDist(0,1,ordinal))]+[categoricalDist(0,1,categorical)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[\"{:.2f}\".format(ordDist(0,2,ordinal))]+[categoricalDist(0,2,categorical)]+[binaryDist(0,1,binary)], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[\"{:.2f}\".format(ordDist(1,2,ordinal))]+[categoricalDist(1,2,categorical)]+[binaryDist(0,1,binary)], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[\"{:.2f}\".format(ordDist(2,3,ordinal))]+[categoricalDist(2,3,categorical)]+[binaryDist(0,1,binary)], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[\"{:.2f}\".format(ordDist(3,4,ordinal))]+[categoricalDist(3,4,categorical)]+[binaryDist(0,1,binary)], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(4,5,numerical))]+[\"{:.2f}\".format(ordDist(4,5,ordinal))]+[categoricalDist(4,5,categorical)]+[binaryDist(0,1,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 2.74 2 0.0 v1-v3 0 1.41 2.71 4 0.0 v2-v3 0 1.41 2.69 2 0.0 v3-v4 0 1.41 2.76 3 0.0 v4-v5 0 1.41 2.74 2 0.0 v5-v6 0 1.41 2.78 3 0.0","title":"Jarak binary"},{"location":"TUGAS/#jarak-campuran","text":"def jarak(v1,v2): return ((chordDist(v1,v2,numerical)+ordDist(v1,v2,ordinal)+categoricalDist(v1,v2,categorical)+binaryDist(v1,v2,binary))/4) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[\"{:.2f}\".format(ordDist(0,1,ordinal))]+[categoricalDist(0,1,categorical)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[\"{:.2f}\".format(ordDist(0,2,ordinal))]+[categoricalDist(0,2,categorical)]+[binaryDist(0,1,binary)], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[\"{:.2f}\".format(ordDist(1,2,ordinal))]+[categoricalDist(1,2,categorical)]+[binaryDist(0,1,binary)], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[\"{:.2f}\".format(ordDist(2,3,ordinal))]+[categoricalDist(2,3,categorical)]+[binaryDist(0,1,binary)], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[\"{:.2f}\".format(ordDist(3,4,ordinal))]+[categoricalDist(3,4,categorical)]+[binaryDist(0,1,binary)], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(4,5,numerical))]+[\"{:.2f}\".format(ordDist(4,5,ordinal))]+[categoricalDist(4,5,categorical)]+[binaryDist(0,1,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 2.74 2 0.0 v1-v3 0 1.41 2.71 4 0.0 v2-v3 0 1.41 2.69 2 0.0 v3-v4 0 1.41 2.76 3 0.0 v4-v5 0 1.41 2.74 2 0.0 v5-v6 0 1.41 2.78 3 0.0","title":"Jarak campuran"},{"location":"TUGAS/#tugas-30","text":"","title":"TUGAS 3.0"},{"location":"TUGAS/#naive-bayes-classifier","text":"Classifier adalah model machine learning yang digunakan untuk membedakan objek berdasarkan fitur tertentu. Naive Bayes Classifier adalah machine learning yang menggunakan probabilitas untuk mengklasfikasi objek dimana untuk setiap fitur $X$ sejumlah $n$ : $$ P(\\operatorname{C_k}) = \\frac{\\left(\\prod_{i=1}^n P(X_i|C_k)\\right)P(C_k)}{P(X)} $$ $P$ adalah probabilitas yang muncul. Untuk data numerik $P$ adalah: $$ P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) $$ dimana $v$ adalah nilai dalam fitur, $\\sigma_k$ adalah Standar deviasi dan $\\mu_k$ adalah Rata-rata untuk K (kolom)","title":"Naive Bayes Classifier"},{"location":"TUGAS/#langkah-langkah-training","text":"","title":"Langkah-Langkah Training"},{"location":"TUGAS/#1-ambil-data-set","text":"from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML, display; from tabulate import tabulate def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) # IRIS TRAINING TABLE iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.2 3.5 1.5 0.2 setosa 5 2.3 3.3 1 versicolor 7.7 2.6 6.9 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 4.3 3 1.1 0.1 setosa 4.9 2.4 3.3 1 versicolor 5.5 2.4 3.7 1 versicolor 6.7 3.1 4.7 1.5 versicolor 5 3.5 1.6 0.6 setosa 6.1 3 4.9 1.8 virginica 5 3.5 1.3 0.3 setosa 6.9 3.2 5.7 2.3 virginica 4.9 3 1.4 0.2 setosa 7.2 3.2 6 1.8 virginica 5.4 3 4.5 1.5 versicolor 5.6 2.7 4.2 1.3 versicolor 4.6 3.2 1.4 0.2 setosa 5.8 2.6 4 1.2 versicolor 4.8 3 1.4 0.3 setosa 4.8 3 1.4 0.1 setosa 7.6 3 6.6 2.1 virginica 5.7 4.4 1.5 0.4 setosa 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 7.7 3 6.1 2.3 virginica 5.7 2.9 4.2 1.3 versicolor 6.7 3 5 1.7 versicolor 4.7 3.2 1.6 0.2 setosa 5.4 3.4 1.7 0.2 setosa","title":"1. Ambil data set"},{"location":"TUGAS/#2-sampel-data-untuk-di-tes","text":"test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4]","title":"2. Sampel data untuk di tes"},{"location":"TUGAS/#3-identifikasi-per-grup-class-target-untuk-data-training","text":"dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.18333333 3.56666667 1.51666667 0.31666667] Sigma_s => [0.3250641 0.22509257 0.14719601 0.1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa versicolor ===> Mu_s => [5.89166667 2.76666667 4.13333333 1.26666667] Sigma_s => [0.52476546 0.33393884 0.46188022 0.21461735] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor virginica ===> Mu_s => [6.61666667 3.13333333 5.58333333 2.06666667] Sigma_s => [0.7790826 0.34465617 0.59670814 0.23868326] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica","title":"3. Identifikasi Per Grup Class Target untuk data Training"},{"location":"TUGAS/#5-hitung-probabilitas-prior-dan-likehood","text":"WIP: Probabilitas Evidence masukkan ke hitungan def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4","title":"5. Hitung Probabilitas Prior dan Likehood"},{"location":"TUGAS/#6-rank-tarik-kesimpulan","text":"Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136 print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica Setelah kita sudah menghitung untuk data training kita, kita akan lakukan test lagi untuk data asli kita # ONE FUNCTION FOR CLASSIFIER def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica","title":"6. Rank &amp; Tarik Kesimpulan"},{"location":"TUGAS/#kesimpulan","text":"corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 144 of 150 == 96.000000 %","title":"Kesimpulan"},{"location":"TUGAS/#tugas-40","text":"","title":"TUGAS 4.0"},{"location":"TUGAS/#naive-bayes-classifier_1","text":"Naive Bayes Classifier adalah classifier dimana untuk setiap fitur $X$ sejumlah $n$ : $$ P(\\operatorname{C_k}) = \\frac{\\left(\\prod_{i=1}^n P(X_i|C_k)\\right)P(C_k)}{P(X)} $$ Layman terms: $$ \\operatorname{posterior} = \\frac{\\operatorname{prior}\\times\\operatorname{likehood}}{\\operatorname{evidence}} $$ $P$ adalah probabilitas yang muncul. Untuk data numerik $P$ adalah: $$ P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) $$ dimana $v$ adalah nilai dalam fitur, $\\sigma_k$ adalah Standar deviasi dan $\\mu_k$ adalah Rata-rata untuk K (kolom)","title":"Naive Bayes Classifier"},{"location":"TUGAS/#langkah-langkah-training_1","text":"","title":"Langkah-Langkah Training"},{"location":"TUGAS/#1-ambil-data-set_1","text":"from sklearn import datasets from pandas import * from numpy import * from math import * from IPython.display import HTML, display; from tabulate import tabulate def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) # IRIS TRAINING TABLE iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.2 3.5 1.5 0.2 setosa 5 2.3 3.3 1 versicolor 7.7 2.6 6.9 2.3 virginica 5.1 3.7 1.5 0.4 setosa 5.2 4.1 1.5 0.1 setosa 4.3 3 1.1 0.1 setosa 4.9 2.4 3.3 1 versicolor 5.5 2.4 3.7 1 versicolor 6.7 3.1 4.7 1.5 versicolor 5 3.5 1.6 0.6 setosa 6.1 3 4.9 1.8 virginica 5 3.5 1.3 0.3 setosa 6.9 3.2 5.7 2.3 virginica 4.9 3 1.4 0.2 setosa 7.2 3.2 6 1.8 virginica 5.4 3 4.5 1.5 versicolor 5.6 2.7 4.2 1.3 versicolor 4.6 3.2 1.4 0.2 setosa 5.8 2.6 4 1.2 versicolor 4.8 3 1.4 0.3 setosa 4.8 3 1.4 0.1 setosa 7.6 3 6.6 2.1 virginica 5.7 4.4 1.5 0.4 setosa 5.6 2.9 3.6 1.3 versicolor 6.7 3.1 4.4 1.4 versicolor 7.7 3 6.1 2.3 virginica 5.7 2.9 4.2 1.3 versicolor 6.7 3 5 1.7 versicolor 4.7 3.2 1.6 0.2 setosa 5.4 3.4 1.7 0.2 setosa","title":"1. Ambil data set"},{"location":"TUGAS/#2-sampel-data-untuk-di-tes_1","text":"test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4]","title":"2. Sampel data untuk di tes"},{"location":"TUGAS/#3-identifikasi-per-grup-class-target-untuk-data-training_1","text":"dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.18333333 3.56666667 1.51666667 0.31666667] Sigma_s => [0.3250641 0.22509257 0.14719601 0.1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa versicolor ===> Mu_s => [5.89166667 2.76666667 4.13333333 1.26666667] Sigma_s => [0.52476546 0.33393884 0.46188022 0.21461735] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor virginica ===> Mu_s => [6.61666667 3.13333333 5.58333333 2.06666667] Sigma_s => [0.7790826 0.34465617 0.59670814 0.23868326] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica","title":"3. Identifikasi Per Grup Class Target untuk data Training"},{"location":"TUGAS/#5-hitung-probabilitas-prior-dan-likehood_1","text":"WIP: Probabilitas Evidence masukkan ke hitungan def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4","title":"5. Hitung Probabilitas Prior dan Likehood"},{"location":"TUGAS/#6-rank-tarik-kesimpulan_1","text":"Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136 print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica","title":"6. Rank &amp; Tarik Kesimpulan"},{"location":"TUGAS/#real-test","text":"diberikan variabel dataset dan dataset_classes yang sebagai 'data training' kita, mari kita lakukan itu untuk real Iris data: # ONE FUNCTION FOR CLASSIFIER def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor virginica 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor versicolor 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica virginica 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica virginica 6.1 3 4.9 1.8 virginica virginica 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica versicolor 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica virginica 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica","title":"Real Test"},{"location":"TUGAS/#kesimpulan_1","text":"corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 144 of 150 == 96.000000 %","title":"Kesimpulan"},{"location":"TUGAS/#tugas-50","text":"","title":"TUGAS 5.0"}]}